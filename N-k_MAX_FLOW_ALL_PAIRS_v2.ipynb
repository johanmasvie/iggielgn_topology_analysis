{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import utils_v2 as utils\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_objects/G_simple_directed.pickle', 'rb') as f:\n",
    "    G_simple_directed = pickle.load(f)\n",
    "    G_simple_directed.name = 'G_simple_directed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# N-k MAX FLOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W(G, global_nodes_lst):\n",
    "    \"\"\"\n",
    "    Computes all-pairs flow matrix W of the network.\n",
    "    \n",
    "    Parameters:\n",
    "        G: A NetworkX MultiDiGraph\n",
    "\n",
    "    Returns:\n",
    "        flow_matrix: 2D numpy array representing the flow matrix\n",
    "        node_indices: Dictionary mapping nodes to their corresponding indices\n",
    "    \"\"\"\n",
    "    num_nodes = len(global_nodes_lst)\n",
    "    node_indices = {node: i for i, node in enumerate(global_nodes_lst)}\n",
    "    flow_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    tot_flow = 0\n",
    "    for i in tqdm(range(num_nodes), desc=\"Computing flow matrix W\"):\n",
    "\n",
    "        source = global_nodes_lst[i]\n",
    "\n",
    "        for j in range(num_nodes):\n",
    "\n",
    "            sink = global_nodes_lst[j]\n",
    "\n",
    "            if source != sink and source in G and sink in G:\n",
    "                if nx.has_path(G, source, sink):\n",
    "                    flow_val, flow_dict = nx.maximum_flow(G, source, sink, capacity=\"max_cap_M_m3_per_d\", flow_func=nx.algorithms.flow.dinitz)\n",
    "\n",
    "                    flow_matrix[i, j] = flow_val\n",
    "                    tot_flow += flow_val\n",
    "            else:\n",
    "                flow_matrix[i, j] = 0           \n",
    "\n",
    "    return flow_matrix, node_indices, tot_flow / num_nodes\n",
    "\n",
    "\n",
    "def W_c(_flow_matrix, target, node_indices):\n",
    "    \"\"\"\n",
    "    Computes the flow matrix W_c after removing a node.\n",
    "    Defined in Cai et al. (2021) as the original flow matrix of the network after removing entry corresponding to the removed node.\n",
    "\n",
    "    Parameters:\n",
    "        flow_matrix: Flow matrix of the original graph\n",
    "        target: Target can be either a single node or an edge in the form (v1, v2)\n",
    "        node_indices: Dictionary mapping nodes to their indices in the flow matrix\n",
    "\n",
    "    Returns:\n",
    "        flow_matrix_c: Flow matrix after removing the specified node\n",
    "        flow_matrix: Modified flow matrix\n",
    "    \"\"\"\n",
    "\n",
    "    flow_matrix = _flow_matrix.copy()\n",
    "\n",
    "    if isinstance(target, (set,tuple)) and len(target) == 2:\n",
    "        # Target is an edge in the form (v1, v2)\n",
    "        v1, v2 = target\n",
    "        index_v1 = node_indices.get(v1, None)\n",
    "        index_v2 = node_indices.get(v2, None)\n",
    "\n",
    "        if index_v1 is not None and index_v2 is not None:\n",
    "            flow_matrix[index_v1, index_v2] = 0\n",
    "            flow_matrix[index_v2, index_v1] = 0\n",
    "    \n",
    "    else:\n",
    "        removed_node_index = node_indices.get(target, None)\n",
    "\n",
    "        if removed_node_index is not None and removed_node_index < flow_matrix.shape[0]:\n",
    "            flow_matrix = np.delete(flow_matrix, removed_node_index, axis=0)\n",
    "            flow_matrix = np.delete(flow_matrix, removed_node_index, axis=1)\n",
    "\n",
    "    return flow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_capacity_robustness(G_, heuristic='random', remove='node', k_removals=150, n_benchmarks = 20, flow_func=nx.algorithms.flow.dinitz):\n",
    "    \"\"\" \n",
    "    Computes the n-k capacity robustness based on maximum flow of a graph\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the graph\n",
    "    G = G_.copy()\n",
    "    \n",
    "    # Instantiate list of all nodes in the graph\n",
    "    global_nodes_lst = list(G.nodes())\n",
    "\n",
    "    # Get all-pairs flow matrix W of the network\n",
    "    flow_matrix, node_indices, flow_val_init = W(G, global_nodes_lst)\n",
    "\n",
    "    # Instantiate the results dataframe\n",
    "    results_df = pd.DataFrame(columns=['max_flow_value', 'capacity_robustness_max_flow', 'heuristic', 'removed_entity'])\n",
    "    results_df.loc[0] = [flow_val_init, 1, None, None]\n",
    "\n",
    "\n",
    "    # Helper function to perform a targeted removal   \n",
    "    def perform_targeted_removal(G, heuristic, target, flow_matrix, _node_indices, results_df):\n",
    "        \n",
    "        if remove == 'edge':\n",
    "            G.remove_edge(*target)\n",
    "        else:\n",
    "            target = target[0]\n",
    "            G.remove_node(target)\n",
    "\n",
    "        # Calculate the flow matrix W_c after removing the node or edge\n",
    "        W_c_ = W_c(flow_matrix, target, _node_indices)\n",
    "\n",
    "        W_c_prime, node_indices, current_flow_val = W(G, global_nodes_lst)\n",
    "\n",
    "        target = target if remove == 'node' else set(target)\n",
    "\n",
    "        results_df.loc[k] = [current_flow_val, np.sum(W_c_prime) / np.sum(W_c_), heuristic, target]\n",
    "\n",
    "        return G, W_c_, node_indices\n",
    "\n",
    "    # Heuristic specific initializations\n",
    "    if heuristic == 'random':\n",
    "        G_lst = [G.copy() for _ in range(n_benchmarks)]\n",
    "        G_node_indices_lst = [node_indices.copy() for _ in range(n_benchmarks)]\n",
    "        G_flow_matrix_lst = [flow_matrix for _ in range(n_benchmarks)]\n",
    "\n",
    "    observed_min_cutset_edge_counts = {}\n",
    "\n",
    "    # N-k capacity robustness calculation\n",
    "    for k in tqdm(range(1, k_removals + 1), desc='N-k capacity robustness'):\n",
    "\n",
    "        if heuristic == 'random':\n",
    "\n",
    "            max_flow_lst, capacity_robustness_lst = [], []\n",
    "\n",
    "            for G_copy, G_flow_matrix, G_node_indices in zip(G_lst, G_flow_matrix_lst, G_node_indices_lst):\n",
    "\n",
    "                # Get a random target to remove\n",
    "                target = random.choice([target for target in (G_copy.nodes() if remove == 'node' else G_copy.edges())])\n",
    "                G_copy.remove_edge(*target) if remove == 'edge' else G_copy.remove_node(target)\n",
    "                \n",
    "                # Calculate W_c and W_c_prime after removing the node or edge\n",
    "                G_flow_matrix = W_c(G_flow_matrix, target, G_node_indices)\n",
    "                G_W_c_prime, G_node_indices, current_flow_val = W(G_copy, global_nodes_lst)\n",
    "\n",
    "                # Append the results to the lists for the current iteration\n",
    "                capacity_robustness_lst.append(np.sum(G_W_c_prime) / np.sum(G_flow_matrix))\n",
    "                max_flow_lst.append(current_flow_val)\n",
    "            \n",
    "            target = target if remove == 'node' else set(target)\n",
    "            results_df.loc[k] = [np.mean(max_flow_lst), np.mean(capacity_robustness_lst), 'random', target]\n",
    "        \n",
    "        elif heuristic == 'load_rate':\n",
    "            target_df = utils.max_flow_edge_count(G, count_or_flow='load_rate')\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'load_rate', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "        \n",
    "\n",
    "        elif heuristic == 'max_flow_edge_count':\n",
    "            target_df = utils.max_flow_edge_count(G)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'max_flow_edge_count', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "        elif heuristic == 'max_flow':\n",
    "            target_df = utils.max_flow_edge_count(G, count_or_flow='flow')\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'max_flow_edge_flows', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)   \n",
    "\n",
    "        elif heuristic == 'min_cutset_edge_count':\n",
    "            target_df, observed_min_cutset_edge_counts = utils.edge_cutset_count(G, observed_min_cutset_edge_counts.copy(), k)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "\n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'min_cutset_edge_count', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "        elif heuristic == 'wfcr':\n",
    "            target_df = utils.weighted_flow_capacity_rate(G)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "\n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'wfcr', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid heuristic\")\n",
    "\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" random_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='random', remove='node')\n",
    "random_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/random_node_removal_df.pkl')\n",
    "random_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/random_node_removal_df.pkl') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "146m \n",
    "\"\"\"\n",
    "load_rate_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='load_rate', remove='node')\n",
    "load_rate_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/load_rate_node_removal_df.pkl')\n",
    "load_rate_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/load_rate_node_removal_df.pkl')\n",
    "utils.results_summary(load_rate_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "129m\n",
    "\"\"\"\n",
    "max_flow_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow', remove='node')\n",
    "max_flow_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_node_removal_df.pkl')\n",
    "max_flow_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_node_removal_df.pkl')\n",
    "utils.results_summary(max_flow_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "144m \n",
    "\"\"\"\n",
    "max_flow_edge_count_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow_edge_count', remove='node')\n",
    "max_flow_edge_count_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_count_node_removal_df.pkl')\n",
    "max_flow_edge_count_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_count_node_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_count_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "126m\n",
    "\"\"\"\n",
    "wfcr_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='wfcr', remove='node')\n",
    "wfcr_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/wfcr_node_removal_df.pkl')\n",
    "wfcr_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/wfcr_node_removal_df.pkl')\n",
    "utils.results_summary(wfcr_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_heuristic_comparison_biplot([load_rate_node_removal_df, max_flow_node_removal_df, max_flow_edge_count_node_removal_df, wfcr_node_removal_df], 'N-k max flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_edge_removal_df = flow_capacity_robustness(G_simple_directed, n_benchmarks=5, heuristic='random', remove='edge')\n",
    "# random_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/random_edge_removal_df.pkl')\n",
    "random_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/random_edge_removal_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_rate_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='load_rate', remove='edge')\n",
    "# load_rate_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/load_rate_edge_removal_df.pkl')\n",
    "load_rate_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/load_rate_edge_removal_df.pkl')\n",
    "utils.results_summary(load_rate_edge_removal_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_flow_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow', remove='edge')\n",
    "# max_flow_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_removal_df.pkl')\n",
    "max_flow_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_flow_edge_count_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow_edge_count', remove='edge')\n",
    "# max_flow_edge_count_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_count_edge_removal_df.pkl')\n",
    "max_flow_edge_count_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/max_flow_edge_count_edge_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_count_edge_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wfcr_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='wfcr', remove='edge')\n",
    "# wfcr_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index_v2/wfcr_edge_removal_df.pkl')\n",
    "wfcr_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index_v2/wfcr_edge_removal_df.pkl')\n",
    "utils.results_summary(wfcr_edge_removal_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_heuristic_comparison_biplot([random_edge_removal_df, load_rate_edge_removal_df, max_flow_edge_removal_df, max_flow_edge_count_edge_removal_df, wfcr_edge_removal_df], 'N-k max flow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
