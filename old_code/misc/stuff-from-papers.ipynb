{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open('graph_objects/scigrid.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "    G.name = 'SciGrid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Network Robustness Analysis Based on Maximum Flow (Cai et al., 2021) \n",
    "Link: https://www.frontiersin.org/articles/10.3389/fphy.2021.792410/full\n",
    "#### *Capacity Robustness Based on Maximum Flow*\n",
    "The network maximum flow matrix *W* is defined as the matrix consisting of the maximum flow values between all pairs of nodes in the network:\n",
    "\n",
    "$\n",
    "W = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0 & c_{f_{max}}(v_1, v_2) & \\cdots & c_{f_{max}}(v_1, v_N) \\\\\n",
    "c_{f_{max}}(v_2, v_1) & 0 & \\cdots & c_{f_{max}}(v_2, v_N) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c_{f_{max}}(v_N, v_1) & c_{f_{max}}(v_N, v_2) & \\cdots & 0 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "$N$ and $V$ denote the size of the network and the set of nodes, respectively. $V_d$ is defined as the set of damaged nodes, $N_d$ is the number of nodes in $V_d$. $V_s$ is the number of remaining nodes. \n",
    "\n",
    "$G = (V, E, c)$ denotes the undamaged network. $G^*_s = (V_s, E_s, c_s)$ denotes the damaged network.\n",
    "\n",
    "Based on $W$, $W_c$ is defined as the matrix after removing the nodes in the set $V_d$ from the maximum flow matrix $W$:\n",
    "\n",
    "$ \n",
    "W_c = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0 & c_{f_{max}}(v_i, v_i+1) & \\cdots & c_{f_{max}}(v_i, v_{i+N_s-1}) \\\\\n",
    "c_{f_{max}}(v_{i+1}, v_i) & 0 & \\cdots & c_{f_{max}}(v_{i+1}, v_{i+N_s-1}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c_{f_{max}}(v_{i+N_s-1}, v_i) & c_{f_{max}}(v_{i+N_s-1}, v_{i+1}) & \\cdots & 0 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "$W^*_c$ is defined as the maxium flow matrix recomputed from the damaged network $G^*_s$:\n",
    "\n",
    "$ \n",
    "W^*_c = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0 & c^*_{f_{max}}(v_i, v_i+1) & \\cdots & c^*_{f_{max}}(v_i, v_{i+N_s-1}) \\\\\n",
    "c^*_{f_{max}}(v_{i+1}, v_i) & 0 & \\cdots & c^*_{f_{max}}(v_{i+1}, v_{i+N_s-1}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c^*_{f_{max}}(v_{i+N_s-1}, v_i) & c^*_{f_{max}}(v_{i+N_s-1}, v_{i+1}) & \\cdots & 0 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "Finally, the flow capacity robustness $C$ is defined as:\n",
    "\n",
    "$\n",
    "C = \\frac{\\sum(W^*_c)}{\\sum(W_c)}\n",
    "$\n",
    "\n",
    "Cai et al. provides a method for systematically testing the performance of the network in a max-flow context, but does not instruct on the removal heuristic itself (i.e., the identification of critical network components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Flow recovery pseudo-code*\n",
    "\n",
    "    Program Network Recovery\n",
    "        For vj in Vs\n",
    "            For vi in Vd\n",
    "                If is Adjacent (vi, vj)  True\n",
    "                    add node vi to network G\n",
    "                    add edge {vi, vj} to network G\n",
    "                End If\n",
    "            End For\n",
    "        End For\n",
    "    End Network Recovery\n",
    "\n",
    "vj and ni are adjacent nodes. Vd set of damaged nodes. Vs set of remaining nodes after n-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def flow_recovery(results_df_, plot_recovery=True):\n",
    "    \"\"\"\n",
    "    Run the recovery algorithm on the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the results dataframe\n",
    "    results_df = results_df_.copy()\n",
    "\n",
    "    # Instantiate the recovery (results) dataframe\n",
    "    recovery_df = pd.DataFrame(columns=['damage_rate', 'flow_recovery', 'flow_improvement_rate', 'recovered_grid_size'])\n",
    "\n",
    "    # Helper function to recover the network\n",
    "    def recover(df_, iteration):\n",
    "        \"\"\"\n",
    "        Flow recovery algorithm to recover the network after damage.\n",
    "        Defined in Cai et al. (2021).  \n",
    "        \"\"\"\n",
    "\n",
    "        df = df_.copy()\n",
    "\n",
    "        damaged_network = df.loc[iteration, 'network_state'].copy()\n",
    "        undamaged_network = df.loc[0, 'network_state'].copy()\n",
    "\n",
    "        # Set of remaining nodes\n",
    "        Vs = list(damaged_network.nodes())\n",
    "\n",
    "        # Set of damanged (removed) nodes\n",
    "        Vd = list(undamaged_network.nodes() - damaged_network.nodes())\n",
    "\n",
    "        reinstated_nodes = []\n",
    "\n",
    "        # Flow recovery\n",
    "        # First, add nodes to reinstate network structure\n",
    "        for vj in Vs:\n",
    "            for vi in Vd:\n",
    "                if (vi, vj) in undamaged_network.edges():\n",
    "                    damaged_network.add_node(vi, pos=undamaged_network.nodes[vi]['pos'], is_country_node=False, recovered_entity=True)\n",
    "                    damaged_network.add_edge(vi, vj, capacity=undamaged_network[vi][vj]['capacity'], recovered_entity=True)\n",
    "                    reinstated_nodes.append(vi)\n",
    "                if (vj, vi) in undamaged_network.edges():\n",
    "                    damaged_network.add_node(vi, pos=undamaged_network.nodes[vi]['pos'], is_country_node=False, recovered_entity=True)\n",
    "                    damaged_network.add_edge(vj, vi, capacity=undamaged_network[vj][vi]['capacity'], recovered_entity=True)\n",
    "                    reinstated_nodes.append(vi)\n",
    "\n",
    "        # Add edges between set of reinstated nodes\n",
    "        for vi in reinstated_nodes:\n",
    "            for vj in reinstated_nodes:\n",
    "                if (vi, vj) in undamaged_network.edges():\n",
    "                    damaged_network.add_edge(vi, vj, capacity=undamaged_network[vi][vj]['capacity'], recovered_entity=True)\n",
    "                if (vj, vi) in undamaged_network.edges():\n",
    "                    damaged_network.add_edge(vj, vi, capacity=undamaged_network[vj][vi]['capacity'], recovered_entity=True)\n",
    "        \n",
    "        # Add edges between reinstated nodes and (previously) remaining nodes\n",
    "        for vi in reinstated_nodes:\n",
    "            for vj in Vs:\n",
    "                if (vi, vj) in undamaged_network.edges():\n",
    "                    damaged_network.add_edge(vi, vj, capacity=undamaged_network[vi][vj]['capacity'], recovered_entity=True)\n",
    "                if (vj, vi) in undamaged_network.edges():\n",
    "                    damaged_network.add_edge(vj, vi, capacity=undamaged_network[vj][vi]['capacity'], recovered_entity=True)\n",
    "\n",
    "        # Damaged network now undergone recovery\n",
    "        recovered_network = damaged_network\n",
    "\n",
    "        damaged_flow_val = df.loc[iteration, 'max_flow_value']\n",
    "        recovered_flow_val, _, _ = utils.max_flow(recovered_network, df.loc[0, 'sources'], df.loc[0, 'sinks'], show_plot=False)\n",
    "\n",
    "        return df.loc[0, 'max_flow_value'], damaged_flow_val, recovered_flow_val, recovered_network\n",
    "\n",
    "    init_num_nodes = results_df.loc[0, 'network_state'].number_of_nodes()\n",
    "\n",
    "    for i in tqdm(range(1, results_df.shape[0]), desc='Recovery process'):\n",
    "        optimal_max_flow, damaged_flow_val, recovered_flow_val, recovered_network = recover(results_df, iteration=i)\n",
    "\n",
    "        damage_rate = 1 - results_df.loc[i, 'network_state'].number_of_nodes() / init_num_nodes\n",
    "        flow_recovery = recovered_flow_val / optimal_max_flow\n",
    "        flow_improvement_rate = None if damaged_flow_val == 0 else sigmoid(recovered_flow_val/damaged_flow_val)\n",
    "        recovered_grid_size = recovered_network.number_of_nodes() / init_num_nodes\n",
    "\n",
    "        recovery_df.loc[i] = [damage_rate, flow_recovery, flow_improvement_rate, recovered_grid_size]\n",
    "\n",
    "    if plot_recovery:\n",
    "        reversed_recovery_df = recovery_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(reversed_recovery_df['damage_rate'], reversed_recovery_df['flow_recovery'], label='flow recovery (relative to optimal)')\n",
    "        plt.plot(reversed_recovery_df['damage_rate'], reversed_recovery_df['flow_improvement_rate'], label='flow improvement (relative to sub-optimal flow incurred by given grid damage)')\n",
    "        plt.plot(reversed_recovery_df['damage_rate'], reversed_recovery_df['recovered_grid_size'], label='size of recovered grid (relative to initial grid size)')\n",
    "        \n",
    "        plt.xlabel('Grid damage rate (pct. nodes removed)')\n",
    "        plt.ylabel('Recovery')\n",
    "        plt.title('Flow recovery')\n",
    "\n",
    "        # Reverse the x-axis scale\n",
    "        plt.gca().invert_xaxis()\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return recovery_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Vulnerability analysis method based on risk assessment for gas transmission capabilities of natural gas pipeline networks (Wang et al., 2021)\n",
    "\n",
    "Link: https://www.sciencedirect.com/science/article/pii/S0951832021006384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Node Importance\"\n",
    "The node degree and gas transmission capacity are used to characterize the Node Importance. Weights are set for the node degree and gas transmission capacity characteristics\n",
    "\n",
    "The importance of node $i$, denoted as $\\lambda_i$, is calculated as follows:\n",
    "\n",
    "$$\\lambda_i = w_1*B_i + w_2*Q_i$$\n",
    "where:\n",
    "- $\\lambda_i$ represents the importance of node $i$\n",
    "- $B_i$ is the degree of the node, which is the ratio of the number of edges connected by $i$ th node to the number of edges of the node with the largest number of connected edges\n",
    "- $Q_i$ is the gas transmission capacity characteristic of the $i$ th node, which is the ratio of the node's capacity to the pipeline network's capacity\n",
    "- $w_1$ and $w_2$ are weights such that $w_1 + w_2 = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_importance(G, w1, w2):\n",
    "    # Calculate the degree of each node\n",
    "    node_degrees = dict(G.degree())\n",
    "    max_degree = max(node_degrees.values())\n",
    "    normalized_degrees = {node: degree / max_degree for node, degree in node_degrees.items()}\n",
    "\n",
    "    # Calculate the gas transmission capacity of each node\n",
    "    node_capacities = {node: sum(data.get('capacity', 0) for _, _, data in G.edges(node, data=True)) for node in G.nodes()}\n",
    "    max_capacity = max(node_capacities.values())\n",
    "    normalized_capacities = {node: capacity / max_capacity for node, capacity in node_capacities.items()}\n",
    "\n",
    "    # Calculate the importance of each node\n",
    "    node_importance = {node: w1 * normalized_degrees[node] + w2 * normalized_capacities[node] for node in G.nodes()}\n",
    "    node_name = {node: data.get('name', '') for node, data in G.nodes(data=True)}\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    df = pd.DataFrame.from_dict(node_importance, orient='index', columns=['nri']).reset_index().rename(columns={'index': 'node'}).sort_values(by='nri', ascending=False)\n",
    "    df['name'] = df['node'].map(node_name)\n",
    "\n",
    "    df = df.reindex(columns=['node', 'name', 'nri'])    \n",
    "    return df\n",
    "\n",
    "# Call the function with weights w1 and w2\n",
    "node_importance_df = calculate_node_importance(G, w1=0.5, w2=0.5)\n",
    "node_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Pipeline Importance\"\n",
    "The Pipeline Importance E includes two indicators: the edge loss rate and weighted flow capacity rate.\n",
    "\n",
    "The importance of pipeline $E$, denoted as $E$, is calculated as follows:\n",
    "\n",
    "$$E = z_1*G_{i,j} + z_2*Q_{(i,j)}^{FCR}$$\n",
    "\n",
    "where:\n",
    "- $E$ represents the importance of the pipeline\n",
    "- $G_{i,j}$ is the edge loss rate, which is defined as $G_{i,j} = \\frac{L_{i,j}}{L}$, where $L$ represents the number of edges in the initial network and $L_{i,j}$ represents the total edge loss of the network after the edge between node $i$ and node $j$ is attacked\n",
    "- $Q_{(i,j)}^{FCR}$ is the weighted flow capacity rate\n",
    "- $z_1$ and $z_2$ are weights such that $z_1 + z_2 = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things:\n",
    "\"Risk indicators for pipeline network components\", i.e., performance of network at global level. The below indicators are plugged into the preference utility function to obtain the calculation formulas for the severity of consequences of the three types of risk indicators.\n",
    "\n",
    "*The vulnerability of pipeline network is divided into the node vulnerability and pipeline vulnerability. It is the product of the risk value and importance.*\n",
    "\n",
    "**Decreased pipeline network connectivity index**\n",
    "$$I_L = 1-C_L$$\n",
    "where $I_L$ is the decrease rate of the connectivity; 1 represents the connection reliability of the pipe network in the normal state; and $C_L$ is the reduction in connection reliability of the pipeline network under the failure state\n",
    "\n",
    "**Reduction of gas transmission capacity index**\n",
    "$$I_C = [Q_{MA}-Q_{MB}] / Q_{MB}$$\n",
    "where $I_C$ is the reduction of the gas transmission capacity index; $G_{MA}$ is the maximum transmission capacity of the pipe network under the failure state; and $Q_{MB}$ is the maximum transmission capacity before the pipe network failure.\n",
    "\n",
    "**Number of users experiencing interruptions index**\n",
    "$$I_{BZ} = n/N$$\n",
    "where $I_{BZ}$ is the number of users experiencing interruptions index; $n$ is the total number of users suffering gas shortages; and $N$ is the total number of users served.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Flow-based vulnerability measures for network component importance: Experimentation with preparedness planning (Nicholson et al., 2016)\n",
    "\n",
    "Link: https://www.sciencedirect.com/science/article/pii/S0951832015002562?ref=pdf_download&fr=RR-2&rr=862305915caa0b51\n",
    "\n",
    "### Max Flow Edge Count (I<sub>MFcount</sub>)\n",
    "Measures how often an edge is utilized in all source-sink (s–t) max flow problems. Calculated using the formula:\n",
    "\n",
    "$$I_{\\text{MFcount}}(i, j) = \\frac{1} {n(n-1)} \\sum_{s,t} {\\mu_{st}(i, j)}$$\n",
    "where $\\mu_{st}=1$ if edge $(i,j)$ is used in a given $s-t$ max flow problem, 0 otherwise. Tally divided by the total number of $s-t$ pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_flow_edge_count(G):\n",
    "    nodes = list(G.nodes)\n",
    "    n = len(nodes)\n",
    "\n",
    "    edge_count = {(u, v): 0 for u, v in G.edges}\n",
    "    \n",
    "    for i in tqdm(range(n), desc='Calculating max flow'):\n",
    "\n",
    "        # The current node\n",
    "        source = nodes[i]\n",
    "\n",
    "        # Skip nodes with out-degree 0\n",
    "        if G.out_degree(source) == 0:\n",
    "            continue        \n",
    "\n",
    "        for j in range(i+1, n):\n",
    "\n",
    "            # Node the max flow is calculated to\n",
    "            target = nodes[j]\n",
    "\n",
    "            # Skip nodes with in-degree 0\n",
    "            if G.in_degree(target) == 0:\n",
    "                continue\n",
    "    \n",
    "            flow_value, flow_dict = nx.maximum_flow(G, source, target, flow_func=nx.algorithms.flow.edmonds_karp)\n",
    "                    \n",
    "            for u, flows in flow_dict.items():\n",
    "                for v, flow in flows.items():\n",
    "                    if flow > 0:\n",
    "                        if (u, v) in edge_count:\n",
    "                            edge_count[(u, v)] += 1\n",
    "                        else:\n",
    "                            edge_count[(v, u)] += 1\n",
    "        \n",
    "    \n",
    "    edge_count_raw = {k: v for k, v in edge_count.items()}\n",
    "    edge_count_normalized = {k: v / (n * (n - 1)) for k, v in edge_count.items()}\n",
    "    \n",
    "    edge_count_combined = {k: {'edge': k,'name': G.edges[k]['name'], 'max_flow_edge_count': v, 'max_flow_edge_count_normalized': edge_count_normalized[k]} for k, v in edge_count_raw.items()}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(edge_count_combined, orient='index').reset_index()\n",
    "    df.drop(columns=['level_0', 'level_1'], inplace=True)\n",
    "    \n",
    "    df = df.sort_values(by='max_flow_edge_count', ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_flow_edge_count_df = max_flow_edge_count(G)\n",
    "# max_flow_edge_count_df.to_pickle('results/max_flow_edge_count_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flow_edge_count_df = pd.read_pickle('results/max_flow_edge_count_df.pkl')\n",
    "max_flow_edge_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Cutset Count (I<sub>cutset</sub>)\n",
    "Reflects the total number of times an edge is a member of the min cutset for all s–t pairs. Calculated using the formula:\n",
    "\n",
    "$$I_{\\text{cutset}}(i, j) = \\frac{1} {n(n-1)} \\sum_{s,t} \\delta_{st}(i, j)$$\n",
    "\n",
    "Highlights edges that act as bottlenecks in max flow problems, and their damage can reduce overall flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def edge_cutset_count(G):\n",
    "    n = len(G.nodes)\n",
    "    edge_cutset_count_ = {}\n",
    "\n",
    "    for edge in G.edges:\n",
    "        edge_cutset_count_[edge] = 0\n",
    "\n",
    "    for s in tqdm(G.nodes, desc='Calculating min cutset count'):\n",
    "        for t in G.nodes:\n",
    "            if s != t:\n",
    "                min_cutset = nx.minimum_edge_cut(G, s, t)\n",
    "                for edge in G.edges:\n",
    "                    if edge in min_cutset:\n",
    "                        edge_cutset_count_[edge] += 1\n",
    "        print(s, edge_cutset_count_)\n",
    "\n",
    "    data = [{\n",
    "        'edge': edge,\n",
    "        'name': G.edges[edge]['name'], \n",
    "        'min_cutset_count': value,\n",
    "        'min_cutset_count_normalized': (value / (n * (n - 1))),\n",
    "    } for edge, value in edge_cutset_count_.items()]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by='min_cutset_count', ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "e# dge_cutset_count_df = edge_cutset_count(G)\n",
    "# edge_cutset_count_df.to_pickle('results/edge_cutset_count_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_cutset_count_df = pd.read_pickle('results/edge_cutset_count_df.pkl')\n",
    "edge_cutset_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Flow Capacity Rate (WFCR)\n",
    "An index used to quantify the criticality of an edge. Has been employed by the following papers:\n",
    "- Vulnerability analysis method based on risk assessment for gas transmission capabilities of natural gas pipeline networks (Wang et al., 2021)\n",
    "- A systematic framework of vulnerability analysis of a natural gas pipeline network (Su et al., 2018)\n",
    "- Flow-based vulnerability measures for network component importance: Experimentation with preparedness planning (Nicholson et al., 2016)\n",
    "\n",
    "It was first defined and employed by Nicholson. \n",
    "> \"The flow capacity rate (FCR) quantifies how close a given edge is to becoming a potential bottleneck based on flow amount and capacity. If an edge is significantly underutilized with respect to its capacity, then it is inherently robust to disruptions that reduce capacity. [...] An edge with a high flow capacity rate is more likely to become a bottleneck than an edge with a lower value, but the expected impact to the overall network performance should also be a function of the expected contribution of the given edge [...]\"\n",
    "\n",
    "$$I_{\\text{WFCR}}(i,j) = \\frac{1} {n(n-1) \\sum_{s,t}\\omega_{st}} \\sum_{s,t} \\frac{[\\omega_{st}(i,j)]^2} {c_{ij}}$$\n",
    "\n",
    "In the formula for $I_{\\text{WFCR}}(i,j)$:\n",
    "- $\\omega_{st}(i,j)$ denotes the maximum flow from source $s$ to target $t$ passing through edge $(i,j)$\n",
    "- $c_{ij}$ denotes the capacity of the pipeline between nodes $i$ and $j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WFCR(G):\n",
    "    nodes = list(G.nodes)\n",
    "    n = len(nodes)\n",
    "\n",
    "    edge_WFCR = {}\n",
    "    tot_flow = 0\n",
    "\n",
    "    for i in tqdm(range(n), desc='Calculating WFCR'):\n",
    "        for j in range(i + 1, n):\n",
    "            \n",
    "            flow_value, flow_dict = nx.maximum_flow(G, nodes[i], nodes[j], flow_func=nx.algorithms.flow.edmonds_karp)\n",
    "            tot_flow += flow_value\n",
    "            \n",
    "            for u, flows in flow_dict.items():\n",
    "                for v, flow in flows.items():\n",
    "                    if flow > 0:\n",
    "                        if (u, v) in G.edges:\n",
    "                            capacity = G.edges[(u, v)]['capacity']\n",
    "                        else:\n",
    "                            capacity = G.edges[(v, u)]['capacity']\n",
    "                        \n",
    "                        if capacity > 0:  \n",
    "                            edge_WFCR[(u, v)] = edge_WFCR.get((u, v), 0) + (flow ** 2) / capacity\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "    if tot_flow == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = [{\n",
    "        'edge': k,\n",
    "        'name': G.edges[k]['name'], \n",
    "        'wfcr': v / (n * (n - 1) * tot_flow),\n",
    "    } for k, v in edge_WFCR.items()]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by='wfcr', ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# wfcr_df = WFCR(G)\n",
    "# wfcr_df.to_pickle('results/wfcr_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfcr_df = pd.read_pickle('results/wfcr_df.pkl')\n",
    "wfcr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Network Robustness Index: A new method for identifying critical links and evaluating the performance of transportation networks (Scott et al., 2006)\n",
    "\n",
    "Link: https://www.sciencedirect.com/science/article/pii/S0966692305000694\n",
    "\n",
    "### Network Robustness Index (NRI)\n",
    "We introduce the Network Robustness Index (NRI) to assess the critical importance of a highway segment within a network. The NRI measures the change in travel-time cost resulting from rerouting all traffic if the segment becomes unusable.\n",
    "\n",
    "Let:\n",
    "- $x_a$: flow (traffic volume) on link $a$\n",
    "- $t_a$: travel time on link $a$, where $t_a = t_a(x_a)$ represents the link performance function or volume-delay curve\n",
    "\n",
    "The NRI is computed based on the relationship between traffic flow and travel time, providing a more realistic evaluation compared to assumptions of no traffic and free-flow speeds.\n",
    "\n",
    "The system-wide, travel-time cost of removing the link $c_a$ is given by:\n",
    "$$c_a = \\sum_{a} t_a \\cdot x_a \\cdot \\delta_a$$\n",
    "\n",
    "where $\\delta_a$ is 1 if link $a$ is not the link removed.\n",
    "\n",
    "The cost is compared to the base case, given by:\n",
    "$$q_a = c_a - c$$\n",
    "where\n",
    "$$c = \\sum_{a} t_a \\cdot x_a$$\n",
    "and $q_a$ is the value of the NRI for link $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Correlation analysis of different vulnerability metrics on power grids (Ouyang et al., 2013)\n",
    "\n",
    "Link: https://www.sciencedirect.com/science/article/pii/S0378437113010133\n",
    "\n",
    "(Correlation analysis performed on the below vulnerability metrics)\n",
    "\n",
    "### Efficiency-based Vulnerability (VE)\n",
    "Measures the normalized average inverse geodesic path distance, assessing the change in efficiency after a damage event.\n",
    "\n",
    "Efficiency (E): $$ E = \\frac{1} {N(N-1)} \\sum_{i \\neq j} \\frac{1}{d_{ij}} $$\n",
    "VE: $$ VE = \\frac{E_{\\text{norm}} - E_{\\text{damg}}} {E_{\\text{norm}}} $$\n",
    "\n",
    "### Source–Demand Efficiency-based Vulnerability (VSDE)\n",
    "Evaluates the change in efficiency, considering the shortest path between generators and load substations after a damage event.\n",
    "\n",
    "Source–demand efficiency (SDE): $$ SDE = \\frac{1} {Ng \\cdot Nd} \\sum_{i \\in Ng, j \\in Nd} \\frac{1}{d_{ij}} $$\n",
    "VSDE: $$ VSDE = \\frac{SDE_{\\text{norm}} - SDE_{\\text{damg}}} {SDE_{\\text{norm}}} $$\n",
    "\n",
    "### Largest Component Size-based Vulnerability (VLCS)\n",
    "Quantifies the change in the number of nodes in the largest connected sub-grid after a damage event.\n",
    "\n",
    "Largest Component Size (LCS): $$ LCS = \\frac{Nl} {N} $$\n",
    "VLCS: $$ VLCS = \\frac{LCS_{\\text{norm}} - LCS_{\\text{damg}}} {LCS_{\\text{norm}}} $$\n",
    "\n",
    "### Connectivity Level-based Vulnerability (VCL)\n",
    "Measures the change in the average fraction of generators connected by each load node in the power grid.\n",
    "\n",
    "Connectivity Level (CL): $$ CL = \\frac{\\sum_{i}^{g} Ng_i}{N} $$\n",
    "VCL: $$ VCL = \\frac{CL_{\\text{norm}} - CL_{\\text{damg}}} {CL_{\\text{norm}}} $$\n",
    "\n",
    "### Clustering Coefficient-based Vulnerability (VCC)\n",
    "Assesses the change in the probability that adjacent nodes are connected in the power grid after a damage event.\n",
    "\n",
    "Clustering Coefficient (CC): $$ CC = \\frac{1} {N} \\sum_{i}^{N} ci $$\n",
    "$$ ci = \\frac{\\text{number of existed lines among neighbors for node i}}{\\text{number of line pairs of neighbors for node i}} $$\n",
    "VCC: $$ VCC = \\frac{CC_{\\text{norm}} - CC_{\\text{damg}}} {CC_{\\text{norm}}} $$\n",
    "\n",
    "### Power Supply-based Vulnerability (VPS)\n",
    "Quantifies the change in the amount of power supplied from generators to load substations (blackout size) after a damage event.\n",
    "\n",
    "Power Supply (PS): $$ VPS = \\frac{PS_{\\text{norm}} - PS_{\\text{damg}}} {PS_{\\text{norm}}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Graph Vulnerability and Robustness: A Survey (Freitas et al.)\n",
    "\n",
    "Link: https://arxiv.org/pdf/2105.00419.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Bottlenecks Identification and Resilience Improvement of Power Networks in Extreme Events (Tu et al., 2022)\n",
    "\n",
    "Link: https://www.frontiersin.org/articles/10.3389/fphy.2022.941165/full\n",
    "\n",
    "### Congestion Link Identification Algorithm\n",
    "\n",
    "LPS: Largest Power Supply\n",
    "\n",
    "The congestion link identification algorithm aims to enhance power network capacity through iterative greedy search. The process involves:\n",
    "\n",
    "1. **Network Initialization:**\n",
    "   - Initialize parameters and power network topology.\n",
    "   - Simulate extreme disasters by randomly removing 30% of links.\n",
    "\n",
    "2. **Connectivity Detection:**\n",
    "   - Identify connected clusters in G after intense disturbances.\n",
    "   - Unserved components in each cluster Si without generators are removed, setting LPS to 0.\n",
    "\n",
    "3. **Power Adjustment:**\n",
    "   - Improve LPS by adjusting remaining component parameters using interior point optimization based on limited resources after extreme events.\n",
    "\n",
    "4. **Capacity Expansion:**\n",
    "   - For each link (i, j), iteratively expand their capacity.\n",
    "   - If the updated LPS increment ΔLPS is greater than the capacity increment δ, accumulate the capacity increment for the corresponding link Δij.\n",
    "\n",
    "5. **Iteration Steps:**\n",
    "   - Repeat power adjustment and capacity expansion until the capacity of each link reaches its maximum value.\n",
    "\n",
    "6. **Results Output:**\n",
    "   - Select links based on the descending order of Δ.\n",
    "   - Output the set of congestion links and the LPS after quick mode adjustment and elimination of transmission bottlenecks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
