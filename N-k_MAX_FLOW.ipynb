{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_objects/G_simple_directed.pickle', 'rb') as f:\n",
    "    G_simple_directed = pickle.load(f)\n",
    "    G_simple_directed.name = 'G_simple_directed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# N-k MAX FLOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W(G, global_nodes_lst):\n",
    "    \"\"\"\n",
    "    Computes all-pairs flow matrix W of the network.\n",
    "    \n",
    "    Parameters:\n",
    "        G: A NetworkX MultiDiGraph\n",
    "\n",
    "    Returns:\n",
    "        flow_matrix: 2D numpy array representing the flow matrix\n",
    "        node_indices: Dictionary mapping nodes to their corresponding indices\n",
    "    \"\"\"\n",
    "    num_nodes = len(global_nodes_lst)\n",
    "    node_indices = {node: i for i, node in enumerate(global_nodes_lst)}\n",
    "    flow_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    tot_flow = 0\n",
    "    for i in tqdm(range(num_nodes), desc=\"Computing flow matrix W\"):\n",
    "\n",
    "        source = global_nodes_lst[i]\n",
    "\n",
    "        for j in range(num_nodes):\n",
    "\n",
    "            sink = global_nodes_lst[j]\n",
    "\n",
    "            if source != sink and source in G and sink in G:\n",
    "                if nx.has_path(G, source, sink):\n",
    "                    flow_val, flow_dict = nx.maximum_flow(G, source, sink, capacity=\"max_cap_M_m3_per_d\", flow_func=nx.algorithms.flow.dinitz)\n",
    "\n",
    "                    flow_matrix[i, j] = flow_val\n",
    "                    tot_flow += flow_val\n",
    "            else:\n",
    "                flow_matrix[i, j] = 0           \n",
    "\n",
    "    return flow_matrix, node_indices, tot_flow / num_nodes\n",
    "\n",
    "\n",
    "def W_c(_flow_matrix, target, node_indices):\n",
    "    \"\"\"\n",
    "    Computes the flow matrix W_c after removing a node.\n",
    "    Defined in Cai et al. (2021) as the original flow matrix of the network after removing entry corresponding to the removed node.\n",
    "\n",
    "    Parameters:\n",
    "        flow_matrix: Flow matrix of the original graph\n",
    "        target: Target can be either a single node or an edge in the form (v1, v2)\n",
    "        node_indices: Dictionary mapping nodes to their indices in the flow matrix\n",
    "\n",
    "    Returns:\n",
    "        flow_matrix_c: Flow matrix after removing the specified node\n",
    "        flow_matrix: Modified flow matrix\n",
    "    \"\"\"\n",
    "\n",
    "    flow_matrix = _flow_matrix.copy()\n",
    "\n",
    "    if isinstance(target, (set,tuple)) and len(target) == 2:\n",
    "        # Target is an edge in the form (v1, v2)\n",
    "        v1, v2 = target\n",
    "        index_v1 = node_indices.get(v1, None)\n",
    "        index_v2 = node_indices.get(v2, None)\n",
    "\n",
    "        if index_v1 is not None and index_v2 is not None:\n",
    "            flow_matrix[index_v1, index_v2] = 0\n",
    "            flow_matrix[index_v2, index_v1] = 0\n",
    "    \n",
    "    else:\n",
    "        removed_node_index = node_indices.get(target, None)\n",
    "\n",
    "        if removed_node_index is not None and removed_node_index < flow_matrix.shape[0]:\n",
    "            flow_matrix = np.delete(flow_matrix, removed_node_index, axis=0)\n",
    "            flow_matrix = np.delete(flow_matrix, removed_node_index, axis=1)\n",
    "\n",
    "    return flow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_capacity_robustness(G_, heuristic='random', remove='node', k_removals=150, n_benchmarks = 20, flow_func=nx.algorithms.flow.dinitz):\n",
    "    \"\"\" \n",
    "    Computes the n-k capacity robustness based on maximum flow of a graph\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the graph\n",
    "    G = G_.copy()\n",
    "    \n",
    "    # Instantiate list of all nodes in the graph\n",
    "    global_nodes_lst = list(G.nodes())\n",
    "\n",
    "    # Get all-pairs flow matrix W of the network\n",
    "    flow_matrix, node_indices, flow_val_init = W(G, global_nodes_lst)\n",
    "\n",
    "    # Instantiate the results dataframe\n",
    "    results_df = pd.DataFrame(columns=['max_flow_value', 'capacity_robustness_max_flow', 'heuristic', 'removed_entity'])\n",
    "    results_df.loc[0] = [flow_val_init, 1, None, None]\n",
    "\n",
    "\n",
    "    # Helper function to perform a targeted removal   \n",
    "    def perform_targeted_removal(G, heuristic, target, flow_matrix, _node_indices, results_df):\n",
    "        \n",
    "        if remove == 'edge':\n",
    "            G.remove_edge(*target)\n",
    "        else:\n",
    "            target = target[0]\n",
    "            G.remove_node(target)\n",
    "\n",
    "        # Calculate the flow matrix W_c after removing the node or edge\n",
    "        W_c_ = W_c(flow_matrix, target, _node_indices)\n",
    "\n",
    "        W_c_prime, node_indices, current_flow_val = W(G, global_nodes_lst)\n",
    "\n",
    "        target = target if remove == 'node' else set(target)\n",
    "\n",
    "        results_df.loc[k] = [current_flow_val, np.sum(W_c_prime) / np.sum(W_c_), heuristic, target]\n",
    "\n",
    "        return G, W_c_, node_indices\n",
    "\n",
    "    # Heuristic specific initializations\n",
    "    if heuristic == 'random':\n",
    "        G_lst = [G.copy() for _ in range(n_benchmarks)]\n",
    "        G_node_indices_lst = [node_indices.copy() for _ in range(n_benchmarks)]\n",
    "        G_flow_matrix_lst = [flow_matrix for _ in range(n_benchmarks)]\n",
    "\n",
    "    observed_min_cutset_edge_counts = {}\n",
    "\n",
    "    # N-k capacity robustness calculation\n",
    "    for k in tqdm(range(1, k_removals + 1), desc='N-k capacity robustness'):\n",
    "\n",
    "        if heuristic == 'random':\n",
    "\n",
    "            max_flow_lst, capacity_robustness_lst = [], []\n",
    "\n",
    "            for G_copy, G_flow_matrix, G_node_indices in zip(G_lst, G_flow_matrix_lst, G_node_indices_lst):\n",
    "\n",
    "                # Get a random target to remove\n",
    "                target = random.choice([target for target in (G_copy.nodes() if remove == 'node' else G_copy.edges())])\n",
    "                G_copy.remove_edge(*target) if remove == 'edge' else G_copy.remove_node(target)\n",
    "                \n",
    "                # Calculate W_c and W_c_prime after removing the node or edge\n",
    "                G_flow_matrix = W_c(G_flow_matrix, target, G_node_indices)\n",
    "                G_W_c_prime, G_node_indices, current_flow_val = W(G_copy, global_nodes_lst)\n",
    "\n",
    "                # Append the results to the lists for the current iteration\n",
    "                capacity_robustness_lst.append(np.sum(G_W_c_prime) / np.sum(G_flow_matrix))\n",
    "                max_flow_lst.append(current_flow_val)\n",
    "            \n",
    "            target = target if remove == 'node' else set(target)\n",
    "            results_df.loc[k] = [np.mean(max_flow_lst), np.mean(capacity_robustness_lst), 'random', target]\n",
    "        \n",
    "        elif heuristic == 'load_rate':\n",
    "            target_df = utils.max_flow_edge_count(G, count_or_flow='load_rate')\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'load_rate', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "        \n",
    "\n",
    "        elif heuristic == 'max_flow_edge_count':\n",
    "            target_df = utils.max_flow_edge_count(G)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'max_flow_edge_count', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "        elif heuristic == 'max_flow':\n",
    "            target_df = utils.max_flow_edge_count(G, count_or_flow='flow')\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "                    \n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'max_flow_edge_flows', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)   \n",
    "\n",
    "        elif heuristic == 'min_cutset_edge_count':\n",
    "            target_df, observed_min_cutset_edge_counts = utils.edge_cutset_count(G, observed_min_cutset_edge_counts.copy(), k)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "\n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'min_cutset_edge_count', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "        elif heuristic == 'wfcr':\n",
    "            target_df = utils.weighted_flow_capacity_rate(G)\n",
    "\n",
    "            if target_df.empty:\n",
    "                return results_df\n",
    "\n",
    "            G, flow_matrix, node_indices = perform_targeted_removal(G, 'wfcr', target_df.iloc[0].edge, flow_matrix, node_indices, results_df)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid heuristic\")\n",
    "\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing flow matrix W:  57%|█████▋    | 408/710 [01:37<01:12,  4.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m random_node_removal_df \u001b[38;5;241m=\u001b[39m flow_capacity_robustness(G_simple_directed, heuristic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m, remove\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m random_node_removal_df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/max_flow/random_node_removal_df.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m random_node_removal_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/max_flow/random_node_removal_df.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mflow_capacity_robustness\u001b[1;34m(G_, heuristic, remove, k_removals, n_benchmarks, flow_func)\u001b[0m\n\u001b[0;32m     10\u001b[0m global_nodes_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get all-pairs flow matrix W of the network\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m flow_matrix, node_indices, flow_val_init \u001b[38;5;241m=\u001b[39m W(G, global_nodes_lst)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Instantiate the results dataframe\u001b[39;00m\n\u001b[0;32m     16\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_flow_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapacity_robustness_max_flow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheuristic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoved_entity\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mW\u001b[1;34m(G, global_nodes_lst)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m!=\u001b[39m sink \u001b[38;5;129;01mand\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m G \u001b[38;5;129;01mand\u001b[39;00m sink \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mhas_path(G, source, sink):\n\u001b[1;32m---> 27\u001b[0m         flow_val, flow_dict \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mmaximum_flow(G, source, sink, capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_cap_M_m3_per_d\u001b[39m\u001b[38;5;124m\"\u001b[39m, flow_func\u001b[38;5;241m=\u001b[39mnx\u001b[38;5;241m.\u001b[39malgorithms\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mdinitz)\n\u001b[0;32m     29\u001b[0m         flow_matrix[i, j] \u001b[38;5;241m=\u001b[39m flow_val\n\u001b[0;32m     30\u001b[0m         tot_flow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m flow_val\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\anaconda3\\envs\\master_thesis\\Lib\\site-packages\\networkx\\algorithms\\flow\\maxflow.py:159\u001b[0m, in \u001b[0;36mmaximum_flow\u001b[1;34m(flowG, _s, _t, capacity, flow_func, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(flow_func):\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow_func has to be callable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m R \u001b[38;5;241m=\u001b[39m flow_func(flowG, _s, _t, capacity\u001b[38;5;241m=\u001b[39mcapacity, value_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m flow_dict \u001b[38;5;241m=\u001b[39m build_flow_dict(flowG, R)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (R\u001b[38;5;241m.\u001b[39mgraph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow_value\u001b[39m\u001b[38;5;124m\"\u001b[39m], flow_dict)\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\anaconda3\\envs\\master_thesis\\Lib\\site-packages\\networkx\\algorithms\\flow\\dinitz_alg.py:135\u001b[0m, in \u001b[0;36mdinitz\u001b[1;34m(G, s, t, capacity, residual, value_only, cutoff)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdinitz\u001b[39m(G, s, t, capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapacity\u001b[39m\u001b[38;5;124m\"\u001b[39m, residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, value_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find a maximum single-commodity flow using Dinitz' algorithm.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    This function returns the residual network resulting after computing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     R \u001b[38;5;241m=\u001b[39m dinitz_impl(G, s, t, capacity, residual, cutoff)\n\u001b[0;32m    136\u001b[0m     R\u001b[38;5;241m.\u001b[39mgraph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinitz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m R\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\anaconda3\\envs\\master_thesis\\Lib\\site-packages\\networkx\\algorithms\\flow\\dinitz_alg.py:149\u001b[0m, in \u001b[0;36mdinitz_impl\u001b[1;34m(G, s, t, capacity, residual, cutoff)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource and sink are the same node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     R \u001b[38;5;241m=\u001b[39m build_residual_network(G, capacity)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     R \u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\anaconda3\\envs\\master_thesis\\Lib\\site-packages\\networkx\\algorithms\\flow\\utils.py:134\u001b[0m, in \u001b[0;36mbuild_residual_network\u001b[1;34m(G, capacity)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m u, v, attr \u001b[38;5;129;01min\u001b[39;00m edge_list:\n\u001b[1;32m--> 134\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(attr\u001b[38;5;241m.\u001b[39mget(capacity, inf), inf)\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m R\u001b[38;5;241m.\u001b[39mhas_edge(u, v):\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;66;03m# Both (u, v) and (v, u) must be present in the residual\u001b[39;00m\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;66;03m# network.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m             R\u001b[38;5;241m.\u001b[39madd_edge(u, v, capacity\u001b[38;5;241m=\u001b[39mr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='random', remove='node')\n",
    "random_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/random_node_removal_df.pkl')\n",
    "random_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/random_node_removal_df.pkl')\n",
    "random_node_removal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rate_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='load_rate', remove='node')\n",
    "load_rate_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/load_rate_node_removal_df.pkl')\n",
    "load_rate_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/load_rate_node_removal_df.pkl')\n",
    "utils.results_summary(load_rate_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "129m\n",
    "\"\"\"\n",
    "# max_flow_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow', remove='node')\n",
    "# max_flow_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/max_flow_node_removal_df.pkl')\n",
    "max_flow_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/max_flow_node_removal_df.pkl')\n",
    "utils.results_summary(max_flow_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "144m \n",
    "\"\"\"\n",
    "# max_flow_edge_count_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow_edge_count', remove='node')\n",
    "# max_flow_edge_count_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_count_node_removal_df.pkl')\n",
    "max_flow_edge_count_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_count_node_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_count_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "167 \n",
    "\"\"\"\n",
    "# min_cutset_edge_count_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='min_cutset_edge_count', remove='node')\n",
    "# min_cutset_edge_count_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/min_cutset_edge_count_node_removal_df.pkl')\n",
    "min_cutset_edge_count_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/min_cutset_edge_count_node_removal_df.pkl')\n",
    "utils.results_summary(min_cutset_edge_count_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "126m\n",
    "\"\"\"\n",
    "# wfcr_node_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='wfcr', remove='node')\n",
    "# wfcr_node_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/wfcr_node_removal_df.pkl')\n",
    "wfcr_node_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/wfcr_node_removal_df.pkl')\n",
    "utils.results_summary(wfcr_node_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_heuristic_comparison_biplot([max_flow_node_removal_df, max_flow_edge_count_node_removal_df, min_cutset_edge_count_node_removal_df, wfcr_node_removal_df], 'N-k max flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='random', remove='edge')\n",
    "random_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/random_edge_removal_df.pkl')\n",
    "random_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/random_edge_removal_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rate_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='load_rate', remove='edge')\n",
    "load_rate_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/load_rate_edge_removal_df.pkl')\n",
    "load_rate_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/load_rate_edge_removal_df.pkl')\n",
    "utils.results_summary(load_rate_edge_removal_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "220m \n",
    "\"\"\"\n",
    "# max_flow_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow', remove='edge')\n",
    "# max_flow_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_removal_df.pkl')\n",
    "max_flow_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "235m \n",
    "\"\"\"\n",
    "# max_flow_edge_count_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='max_flow_edge_count', remove='edge')\n",
    "# max_flow_edge_count_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_count_edge_removal_df.pkl')\n",
    "max_flow_edge_count_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/max_flow_edge_count_edge_removal_df.pkl')\n",
    "utils.results_summary(max_flow_edge_count_edge_removal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "238m\n",
    "\"\"\"\n",
    "# min_cutset_edge_count_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='min_cutset_edge_count', remove='edge')\n",
    "# min_cutset_edge_count_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/min_cutset_edge_count_edge_removal_df.pkl')\n",
    "min_cutset_edge_count_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/min_cutset_edge_count_edge_removal_df.pkl')\n",
    "utils.results_summary(min_cutset_edge_count_edge_removal_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "226m \n",
    "\"\"\"\n",
    "# wfcr_edge_removal_df = flow_capacity_robustness(G_simple_directed, heuristic='wfcr', remove='edge')\n",
    "# wfcr_edge_removal_df.to_pickle('results/max_flow/all_pairs_flow_index/wfcr_edge_removal_df.pkl')\n",
    "wfcr_edge_removal_df = pd.read_pickle('results/max_flow/all_pairs_flow_index/wfcr_edge_removal_df.pkl')\n",
    "utils.results_summary(wfcr_edge_removal_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_heuristic_comparison_biplot([max_flow_edge_removal_df, max_flow_edge_count_edge_removal_df, min_cutset_edge_count_edge_removal_df, wfcr_edge_removal_df], 'N-k max flow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
